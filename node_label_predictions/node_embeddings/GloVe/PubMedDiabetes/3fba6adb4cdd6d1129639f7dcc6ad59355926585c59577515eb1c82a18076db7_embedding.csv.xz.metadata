{
    "creation_time": 1617526174.6116512,
    "creation_time_human": "2021-04-04 08:49:34",
    "time_delta": 2973.078508615494,
    "time_delta_human": "49 minutes and 33 seconds",
    "file_dump_time": 43.91854381561279,
    "file_dump_time_human": "43 seconds",
    "file_dump_size": 8219056,
    "file_dump_size_human": "8.2 MB",
    "load_kwargs": {},
    "dump_kwargs": {},
    "function_name": "_compute_node_embedding",
    "function_file": "/usr/local/lib/python3.6/dist-packages/embiggen/utils/compute_node_embedding.py:47",
    "args_to_ignore": [
        "devices",
        "verbose"
    ],
    "source": "@Cache(\n    cache_path=[\n        \"node_embeddings/{node_embedding_method_name}/{graph_name}/{_hash}_embedding.csv.xz\",\n        \"node_embeddings/{node_embedding_method_name}/{graph_name}/{_hash}_training_history.csv.xz\",\n    ],\n    args_to_ignore=[\"devices\", \"verbose\"]\n)\ndef _compute_node_embedding(\n    graph: EnsmallenGraph,\n    graph_name: str,  # pylint: disable=unused-argument\n    node_embedding_method_name: str,\n    fit_kwargs: Dict,\n    verbose: bool = True,\n    devices: Union[List[str], str] = None,\n    **kwargs: Dict\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Return embedding computed with required node embedding method.\n\n    Specifically, this method also caches the embedding automatically.\n\n    Parameters\n    --------------------------\n    graph: EnsmallenGraph,\n        The graph to embed.\n    graph_name: str,\n        The name of the graph.\n    node_embedding_method_name: str,\n        The name of the node embedding method to use.\n    fit_kwargs: Dict,\n        Arguments to pass to the fit call.\n    verbose: bool = True,\n        Whether to show loading bars.\n    devices: Union[List[str], str] = None,\n        The devices to use.\n        If None, all GPU devices available are used.\n    **kwargs: Dict,\n        Arguments to pass to the node embedding method constructor.\n        Read the documentation of the selected method.\n\n    Returns\n    --------------------------\n    Tuple with node embedding and training history.\n    \"\"\"\n    # Since the verbose kwarg may be provided also on the fit_kwargs\n    # we normalize the parameter to avoid collisions.\n    verbose = fit_kwargs.pop(\"verbose\", verbose)\n    strategy = tf.distribute.MirroredStrategy(devices=devices)\n    with strategy.scope():\n        # Creating the node embedding model\n        model = get_node_embedding_method(node_embedding_method_name)(\n            graph,\n            support_mirror_strategy=True,\n            **kwargs\n        )\n        # Fitting the node embedding model\n        history = model.fit(\n            verbose=verbose,\n            **fit_kwargs\n        )\n        # Extracting computed embedding\n        node_embedding = model.get_embedding_dataframe()\n    return node_embedding, history\n",
    "backend_metadata": {
        "type": "pandas",
        "columns_types": {
            "0": "float32",
            "1": "float32",
            "2": "float32",
            "3": "float32",
            "4": "float32",
            "5": "float32",
            "6": "float32",
            "7": "float32",
            "8": "float32",
            "9": "float32",
            "10": "float32",
            "11": "float32",
            "12": "float32",
            "13": "float32",
            "14": "float32",
            "15": "float32",
            "16": "float32",
            "17": "float32",
            "18": "float32",
            "19": "float32",
            "20": "float32",
            "21": "float32",
            "22": "float32",
            "23": "float32",
            "24": "float32",
            "25": "float32",
            "26": "float32",
            "27": "float32",
            "28": "float32",
            "29": "float32",
            "30": "float32",
            "31": "float32",
            "32": "float32",
            "33": "float32",
            "34": "float32",
            "35": "float32",
            "36": "float32",
            "37": "float32",
            "38": "float32",
            "39": "float32",
            "40": "float32",
            "41": "float32",
            "42": "float32",
            "43": "float32",
            "44": "float32",
            "45": "float32",
            "46": "float32",
            "47": "float32",
            "48": "float32",
            "49": "float32",
            "50": "float32",
            "51": "float32",
            "52": "float32",
            "53": "float32",
            "54": "float32",
            "55": "float32",
            "56": "float32",
            "57": "float32",
            "58": "float32",
            "59": "float32",
            "60": "float32",
            "61": "float32",
            "62": "float32",
            "63": "float32",
            "64": "float32",
            "65": "float32",
            "66": "float32",
            "67": "float32",
            "68": "float32",
            "69": "float32",
            "70": "float32",
            "71": "float32",
            "72": "float32",
            "73": "float32",
            "74": "float32",
            "75": "float32",
            "76": "float32",
            "77": "float32",
            "78": "float32",
            "79": "float32",
            "80": "float32",
            "81": "float32",
            "82": "float32",
            "83": "float32",
            "84": "float32",
            "85": "float32",
            "86": "float32",
            "87": "float32",
            "88": "float32",
            "89": "float32",
            "90": "float32",
            "91": "float32",
            "92": "float32",
            "93": "float32",
            "94": "float32",
            "95": "float32",
            "96": "float32",
            "97": "float32",
            "98": "float32",
            "99": "float32"
        },
        "index_type": "str",
        "columns_names_type": "int64"
    },
    "parameters": {
        "graph_name": "PubMedDiabetes",
        "node_embedding_method_name": "GloVe",
        "fit_kwargs": {},
        "explore_weight": 2,
        "return_weight": 0.5
    }
}