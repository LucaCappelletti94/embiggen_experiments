{
    "creation_time": 1617387469.2915006,
    "creation_time_human": "2021-04-02 18:17:49",
    "time_delta": 337.47981119155884,
    "time_delta_human": "5 minutes and 37 seconds",
    "file_dump_time": 0.004422664642333984,
    "file_dump_time_human": "0 seconds",
    "file_dump_size": 484,
    "file_dump_size_human": "484 Bytes",
    "load_kwargs": {},
    "dump_kwargs": {},
    "function_name": "_compute_node_embedding",
    "function_file": "/usr/local/lib/python3.6/dist-packages/embiggen/utils/compute_node_embedding.py:47",
    "args_to_ignore": [
        "devices",
        "verbose"
    ],
    "source": "@Cache(\n    cache_path=[\n        \"node_embeddings/{node_embedding_method_name}/{graph_name}/{_hash}_embedding.csv.xz\",\n        \"node_embeddings/{node_embedding_method_name}/{graph_name}/{_hash}_training_history.csv.xz\",\n    ],\n    args_to_ignore=[\"devices\", \"verbose\"]\n)\ndef _compute_node_embedding(\n    graph: EnsmallenGraph,\n    graph_name: str,  # pylint: disable=unused-argument\n    node_embedding_method_name: str,\n    fit_kwargs: Dict,\n    verbose: bool = True,\n    devices: Union[List[str], str] = None,\n    **kwargs: Dict\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Return embedding computed with required node embedding method.\n\n    Specifically, this method also caches the embedding automatically.\n\n    Parameters\n    --------------------------\n    graph: EnsmallenGraph,\n        The graph to embed.\n    graph_name: str,\n        The name of the graph.\n    node_embedding_method_name: str,\n        The name of the node embedding method to use.\n    fit_kwargs: Dict,\n        Arguments to pass to the fit call.\n    verbose: bool = True,\n        Whether to show loading bars.\n    devices: Union[List[str], str] = None,\n        The devices to use.\n        If None, all GPU devices available are used.\n    **kwargs: Dict,\n        Arguments to pass to the node embedding method constructor.\n        Read the documentation of the selected method.\n\n    Returns\n    --------------------------\n    Tuple with node embedding and training history.\n    \"\"\"\n    # Since the verbose kwarg may be provided also on the fit_kwargs\n    # we normalize the parameter to avoid collisions.\n    verbose = fit_kwargs.pop(\"verbose\", verbose)\n    strategy = tf.distribute.MirroredStrategy(devices=devices)\n    with strategy.scope():\n        # Creating the node embedding model\n        model = get_node_embedding_method(node_embedding_method_name)(\n            graph,\n            support_mirror_strategy=True,\n            **kwargs\n        )\n        # Fitting the node embedding model\n        history = model.fit(\n            verbose=verbose,\n            **fit_kwargs\n        )\n        # Extracting computed embedding\n        node_embedding = model.get_embedding_dataframe()\n    return node_embedding, history\n",
    "backend_metadata": {
        "type": "pandas",
        "columns_types": {
            "loss": "float64",
            "lr": "float64"
        },
        "index_type": "int64",
        "columns_names_type": "str"
    },
    "parameters": {
        "graph_name": "CiteSeer",
        "node_embedding_method_name": "CBOW",
        "fit_kwargs": {},
        "explore_weight": 2,
        "return_weight": 0.5,
        "negative_samples": 100
    }
}